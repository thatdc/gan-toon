{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_Toons.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hZ28tC4YF5Yf",
        "7TiVbBaPC3y0",
        "3CDqA-tgC8D6"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByIivfyBq7z5"
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from numpy.random import randn, randint\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "from IPython import display\n",
        "\n",
        "SEED = 7091998\n",
        "tf.random.set_seed(SEED)  \n",
        "\n",
        "# Get current working directory\n",
        "cwd = os.getcwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG86ntRXsV-d",
        "outputId": "15132d2d-d234-46f1-f308-7bd6c160c109"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW9JbUTq-Yl7"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, BatchNormalization, Conv2DTranspose, LeakyReLU, Reshape, Conv2D, LeakyReLU, Dropout, MaxPool2D, GlobalAveragePooling2D, Flatten, Activation, BatchNormalization, UpSampling2D, Input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import Model\n",
        "from PIL import Image\n",
        "from keras.initializers import RandomNormal\n",
        "from datetime import datetime\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzgXRwGRGE5E"
      },
      "source": [
        "!cp \"/content/drive/My Drive/datasets/std_dataset.zip\" \"/content/std_dataset.zip\"\n",
        "!unzip -oq \"/content/std_dataset.zip\" -d \"/content/data/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOm4i70nCzVG"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LLZ0FA6vxSr"
      },
      "source": [
        "in_h, in_w = 4, 4\n",
        "img_h, img_w = 64, 64\n",
        "start_f = 256\n",
        "channels = 3\n",
        "input_shape = (in_h, in_w, start_f)\n",
        "img_shape = (img_h, img_w, channels)\n",
        "latent_dim = 100\n",
        "n_conv = 2\n",
        "\n",
        "dropout_g = 0.4\n",
        "dropout_d = 0.2\n",
        "\n",
        "net_depth = 3\n",
        "leaky_relu_slope = 0.18\n",
        "n_conv = 1\n",
        "\n",
        "g_kernel_size = (5, 5)\n",
        "c_kernel_size = (5, 5)\n",
        "strides_size = (2, 2)\n",
        "pool_size = (2, 2)\n",
        "\n",
        "clip_value = 0.01\n",
        "n_critic = 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ28tC4YF5Yf"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78a2CyHyF8JV"
      },
      "source": [
        "dataset_dir = \"/content/data/std_dataset\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWks-O-iVMLF"
      },
      "source": [
        "class CustomDataset(tf.keras.utils.Sequence):\n",
        "  def __init__(self, dataset_dir):\n",
        "    self.img_list = os.listdir(dataset_dir)\n",
        "    self.dataset_dir = dataset_dir\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.img_list)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    curr_filename = self.img_list[index]\n",
        "    img = Image.open(os.path.join(self.dataset_dir, curr_filename)).convert('RGB')\n",
        "    img = img.resize((64, 64))\n",
        "    img_arr = np.array(img)\n",
        "    img_arr = np.float32(img_arr)\n",
        "    # Normalize to [-1, 1]\n",
        "    img_arr = (img_arr - 127.5) / 127.5\n",
        "\n",
        "    return img_arr\n",
        "  \n",
        "  def getnparr(self):\n",
        "    X = []\n",
        "    for i in range(len(self.img_list)):\n",
        "      x = self.__getitem__(i)\n",
        "      X.append(x)\n",
        "    \n",
        "    return np.asarray(X)\n",
        "\n",
        "dataset = CustomDataset(dataset_dir).getnparr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixbrzTSUWcsR"
      },
      "source": [
        "def generate_real_samples(dataset, n_samples):\n",
        "  # choose random instances\n",
        "  ix = randint(0, len(dataset), n_samples)\n",
        "  # retrieve selected images\n",
        "  X = dataset[ix]\n",
        "  # generate 'real' class labels (1)\n",
        "  y = np.ones((n_samples, 1))\n",
        "  return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkTijAYVsoBy"
      },
      "source": [
        "## Custom classes and functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6c2Oh0Usp_a"
      },
      "source": [
        "import keras.backend as K\n",
        "from keras.constraints import Constraint\n",
        "\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "        return K.mean(y_true * y_pred)\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(lr=0.00005)\n",
        "\n",
        "# clip model weights to a given hypercube\n",
        "class ClipConstraint(Constraint):\n",
        "\t# set clip value when initialized\n",
        "\tdef __init__(self, clip_value):\n",
        "\t\tself.clip_value = clip_value\n",
        "\n",
        "\t# clip model weights to hypercube\n",
        "\tdef __call__(self, weights):\n",
        "\t\treturn K.clip(weights, -self.clip_value, self.clip_value)\n",
        "\n",
        "\t# get the config\n",
        "\tdef get_config(self):\n",
        "\t\treturn {'clip_value': self.clip_value}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TiVbBaPC3y0"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZmELgSQveA2"
      },
      "source": [
        "def make_generator_model(batchnorm=True):\n",
        "    model = tf.keras.Sequential(name='GenToon')\n",
        "    n_nodes = in_h*in_w*start_f\n",
        "    init = RandomNormal(stddev=0.02)\n",
        "    \n",
        "    # Input and reshape\n",
        "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "    model.add(Reshape((in_h, in_w, start_f)))\n",
        "\n",
        "    # Upsampling\n",
        "    for i in range(net_depth+1):\n",
        "      model.add(UpSampling2D())\n",
        "      for j in range(n_conv):\n",
        "        model.add(Conv2D(filters=start_f/2**(i+1), kernel_size=g_kernel_size, padding='same', kernel_initializer=init))\n",
        "        if batchnorm:\n",
        "          model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation('relu'))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Conv2D(filters=3, kernel_size=(3, 3), activation='tanh', padding='same', kernel_initializer=init))\n",
        "\n",
        "    return model\n",
        "\n",
        "make_generator_model().summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wBMtHSDagCu"
      },
      "source": [
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "  # generate points in the latent space\n",
        "  x_input = randn(latent_dim * n_samples)\n",
        "  # reshape into a batch of inputs for the network\n",
        "  x_input = x_input.reshape(n_samples, latent_dim)\n",
        "  return x_input\n",
        "\n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
        "  # generate points in latent space\n",
        "  x_input = generate_latent_points(latent_dim, n_samples)\n",
        "  # predict outputs\n",
        "  X = g_model.predict(x_input)\n",
        "  # create 'fake' class labels (0)\n",
        "  y = np.zeros((n_samples, 1))\n",
        "  return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CDqA-tgC8D6"
      },
      "source": [
        "## Discriminator / Critic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRLYsTGs_qHJ"
      },
      "source": [
        "def make_discriminator_model(batchnorm=False):\n",
        "  model = tf.keras.Sequential(name='ToonCritic')\n",
        "  filters = (start_f) // 2**net_depth\n",
        "  \n",
        "  # weight constraint\n",
        "  const = ClipConstraint(clip_value=clip_value)\n",
        "  init = RandomNormal(stddev=0.02)\n",
        "  \n",
        "  # Input\n",
        "  model.add(Conv2D(filters=filters, kernel_size=c_kernel_size, padding='same', input_shape=img_shape, kernel_initializer=init, kernel_constraint=const))\n",
        "  model.add(LeakyReLU(alpha=leaky_relu_slope))\n",
        "  model.add(Dropout(dropout_d))\n",
        "\n",
        "  filters = filters * 2\n",
        "\n",
        "  # Convolutions\n",
        "  for i in range(net_depth):\n",
        "    for j in range(n_conv):\n",
        "      if j == 0:\n",
        "        strides = (2, 2)\n",
        "      else:\n",
        "        strides = (1, 1)\n",
        "      model.add(Conv2D(filters=filters, strides=strides, kernel_size=c_kernel_size, padding='same', kernel_initializer=init, kernel_constraint=const))\n",
        "      if batchnorm:\n",
        "        model.add(BatchNormalization())\n",
        "      model.add(LeakyReLU(alpha=leaky_relu_slope))\n",
        "    \n",
        "    filters = filters * 2\n",
        "\n",
        "  # FC part\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  opt = tf.keras.optimizers.RMSprop(0.00005)\n",
        "  model.compile(loss=wasserstein_loss, optimizer=opt)\n",
        "  \n",
        "  return model\n",
        "\n",
        "make_discriminator_model().summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxM5rCl9e39b"
      },
      "source": [
        "## WGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HRoyGxae4ut"
      },
      "source": [
        "def create_wgan(generator, critic, opt_critic_weights=None, opt_wgan_weights=None):\n",
        "  # Freeze the critic\n",
        "  critic.trainable=False\n",
        "  # Build model\n",
        "  model = tf.keras.Sequential(name='WGAN')\n",
        "  \n",
        "  # Add the Generator\n",
        "  model.add(generator)\n",
        "  \n",
        "  # Load Critic\n",
        "  if opt_critic_weights is not None:\n",
        "    opt = tf.keras.optimizers.deserialize(opt_critic_weights)\n",
        "    critic.compile(loss=wasserstein_loss, optimizer=opt)\n",
        "  else:\n",
        "    opt = tf.keras.optimizers.RMSprop(lr=0.00005)\n",
        "  model.add(critic)\n",
        "  \n",
        "  # Optional WGAN optimizer state\n",
        "  if opt_wgan_weights is not None:\n",
        "    opt = tf.keras.optimizers.deserialize(opt_wgan_weights)\n",
        "  else:\n",
        "    opt = tf.keras.optimizers.RMSprop(lr=0.00005)\n",
        "  \n",
        "  model.compile(loss=wasserstein_loss, optimizer=opt)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w88YL4otfH0e"
      },
      "source": [
        "fresh_model = True\n",
        "models_path = '/content/drive/My Drive/Colab Notebooks/wgan_models'\n",
        "gen_name = 'generator_model_100.h5'\n",
        "critic_name = 'critic_model_100.h5'\n",
        "critic_optimizer_name = 'optimizer_critic_100.pkl'\n",
        "wgan_optimizer_name = 'optimizer_wgan_100.pkl'\n",
        "\n",
        "if fresh_model:\n",
        "  print('Creating a fresh model...')\n",
        "  generator = make_generator_model(True)\n",
        "  critic = make_discriminator_model(False)\n",
        "\n",
        "  wgan = create_wgan(generator, critic)\n",
        "else:\n",
        "  # Load Generator\n",
        "  generator_path = os.path.join(models_path, gen_name)\n",
        "  print('Loading Generator ...')\n",
        "  generator = tf.keras.models.load_model(generator_path)\n",
        "  # Load Critic\n",
        "  critic_path = os.path.join(models_path, critic_name)\n",
        "  print('Loading Critic ...')\n",
        "  critic = tf.keras.models.load_model(critic_path, custom_objects={'ClipConstraint': ClipConstraint, 'wasserstein_loss': wasserstein_loss})\n",
        "  # Critic Optimizer state\n",
        "  optimizer_path = os.path.join(models_path, critic_optimizer_name)\n",
        "  print('Loading optimizer for Critic ...')\n",
        "  with open(optimizer_path, 'rb') as f:\n",
        "    critic_optimizer = pickle.load(f)\n",
        "  # WGAN Optimzier state\n",
        "  optimizer_path = os.path.join(models_path, wgan_optimizer_name)\n",
        "  print('Loading optimizer for WGAN ...')\n",
        "  with open(optimizer_path, 'rb') as f:\n",
        "    wgan_optimizer = pickle.load(f)\n",
        "  \n",
        "  wgan = create_wgan(generator, critic, opt_critic_weights=critic_optimizer, opt_wgan_weights=wgan_optimizer)\n",
        "\n",
        "wgan.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b-dl047fg29"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCVMn5Ecgj78"
      },
      "source": [
        "# create and save a plot of generated images\n",
        "def save_plot(examples, epoch, n=3):\n",
        "\t# scale from [-1,1] to [0,1]\n",
        "\texamples = (examples + 1) / 2.0\n",
        "\t# plot images\n",
        "\tfor i in range(n * n):\n",
        "\t\t# define subplot\n",
        "\t\tpyplot.subplot(n, n, 1 + i)\n",
        "\t\t# turn off axis\n",
        "\t\tpyplot.axis('off')\n",
        "\t\t# plot raw pixel data\n",
        "\t\tpyplot.imshow(examples[i])\n",
        "\t# save plot to file\n",
        "\tfilename = 'generated_plot_e%03d.png' % (epoch+1)\n",
        "\tpyplot.savefig(os.path.join('/content/drive/My Drive/Colab Notebooks/WGAN_results', filename))\n",
        "\tpyplot.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKZllmtSjjYR"
      },
      "source": [
        "base_path = '/content/drive/My Drive/Colab Notebooks/wgan_models'\n",
        "\n",
        "def plot_and_save(epoch, g_model, d_model, wgan, dataset, latent_dim, n_samples=9):\n",
        "  # prepare fake examples\n",
        "  x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "  # save plot\n",
        "  save_plot(x_fake, epoch)\n",
        "  \n",
        "  # Generator Model\n",
        "  filename_g = 'generator_model_%03d.h5' % (epoch+1)\n",
        "  tf.keras.Model.save(g_model, os.path.join(base_path, filename_g))\n",
        "\n",
        "  # Critic model\n",
        "  filename_d = 'critic_model_%03d.h5' % (epoch+1)\n",
        "  d_model.save(os.path.join(base_path, filename_d), include_optimizer=False)\n",
        "\n",
        "  # Critic optimizer weights\n",
        "  weights_filename = 'optimizer_critic_%03d.pkl' % (epoch+1)\n",
        "  opt_save = os.path.join(base_path, weights_filename)\n",
        "  opt_weights = tf.keras.optimizers.serialize(critic.optimizer)\n",
        "  with open(opt_save, 'wb') as f:\n",
        "    pickle.dump(opt_weights, f)\n",
        "\n",
        "  # WGAN optimizer weights\n",
        "  weights_filename = 'optimizer_wgan_%03d.pkl' % (epoch+1)\n",
        "  opt_save = os.path.join(base_path, weights_filename)\n",
        "  opt_weights = tf.keras.optimizers.serialize(wgan.optimizer)\n",
        "  with open(opt_save, 'wb') as f:\n",
        "    pickle.dump(opt_weights, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPociRoIj0E-"
      },
      "source": [
        "# Functions for training\n",
        "\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "\t# choose random instances\n",
        "\tix = randint(0, dataset.shape[0], n_samples)\n",
        "\t# select images\n",
        "\tX = dataset[ix]\n",
        "\t# generate class labels, -1 for 'real'\n",
        "\ty = -np.ones((n_samples, 1))\n",
        "\treturn X, y\n",
        "\n",
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input\n",
        "\n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "\t# generate points in latent space\n",
        "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
        "\t# predict outputs\n",
        "\tX = generator.predict(x_input)\n",
        "\t# create class labels with 1.0 for 'fake'\n",
        "\ty = np.ones((n_samples, 1))\n",
        "\treturn X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byQW1L7XfSE1"
      },
      "source": [
        "# train the generator and critic\n",
        "def train(dataset, critic, generator, wgan, epochs, batch_size, start_epoch=0):\n",
        "\n",
        "  # main gan training loop\n",
        "  for epoch in range(start_epoch, epochs):\n",
        "    start_time = datetime.now()\n",
        "    steps_per_epoch = dataset.shape[0] // (batch_size)\n",
        "    half_batch = batch_size // 2\n",
        "    for step in range(steps_per_epoch):\n",
        "      \n",
        "      # CRITIC TRAIN\n",
        "      for _ in range(n_critic):\n",
        "        # get randomly selected 'real' samples\n",
        "        X_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "        # update critic model weights\n",
        "        c_loss1 = critic.train_on_batch(X_real, y_real)\n",
        "        # generate 'fake' examples\n",
        "        X_fake, y_fake = generate_fake_samples(generator, latent_dim, half_batch)\n",
        "        # update critic model weights\n",
        "        c_loss2 = critic.train_on_batch(X_fake, y_fake)\n",
        "        c_loss = (c_loss1 + c_loss2) / 2\n",
        "\n",
        "      # GENERATOR TRAIN\n",
        "      # prepare points in latent space as input for the generator\n",
        "      X_gan = generate_latent_points(latent_dim, batch_size)\n",
        "      # create inverted labels for the fake samples\n",
        "      y_gan = -np.ones((batch_size, 1))\n",
        "      # update the generator via the critic's error\n",
        "      g_loss = wgan.train_on_batch(X_gan, y_gan)\n",
        "    \n",
        "    # Summarize epoch results\n",
        "    seconds = (datetime.now() - start_time).total_seconds()\n",
        "    print('Epoch %d >> [critic_loss: %.2f] [generator_loss: %.2f] [time: %d]' % (epoch+1, 1-c_loss, 1-g_loss, seconds))\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "      plot_and_save(epoch, generator, critic, wgan, dataset, latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PEF8ptWgo7_",
        "outputId": "76021ec7-6a2b-4776-f5ab-811afebfdb0e"
      },
      "source": [
        "# train model\n",
        "EPOCHS = 500\n",
        "BS = 256\n",
        "train(dataset, critic, generator, wgan, EPOCHS, BS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 >> [critic_loss: -48.39] [generator_loss: 34525.77] [time: 119]\n",
            "Epoch 2 >> [critic_loss: -12.35] [generator_loss: 2147.63] [time: 120]\n",
            "Epoch 3 >> [critic_loss: 4268.64] [generator_loss: -374.62] [time: 121]\n",
            "Epoch 4 >> [critic_loss: 3669.42] [generator_loss: 22813.02] [time: 119]\n",
            "Epoch 5 >> [critic_loss: 691.72] [generator_loss: 25963.17] [time: 119]\n",
            "Epoch 6 >> [critic_loss: 2067.01] [generator_loss: 4736.51] [time: 119]\n",
            "Epoch 7 >> [critic_loss: 2743.16] [generator_loss: 7541.40] [time: 118]\n",
            "Epoch 8 >> [critic_loss: 711.56] [generator_loss: 10429.52] [time: 118]\n",
            "Epoch 9 >> [critic_loss: -31.39] [generator_loss: 3048.88] [time: 118]\n",
            "Epoch 10 >> [critic_loss: 1192.18] [generator_loss: 3188.65] [time: 118]\n",
            "Epoch 11 >> [critic_loss: 1605.44] [generator_loss: 3419.38] [time: 118]\n",
            "Epoch 12 >> [critic_loss: 356.66] [generator_loss: 1274.45] [time: 118]\n",
            "Epoch 13 >> [critic_loss: 93.75] [generator_loss: 1303.90] [time: 118]\n",
            "Epoch 14 >> [critic_loss: 221.53] [generator_loss: 538.80] [time: 118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YAufZivEE-M"
      },
      "source": [
        "## Test time!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHb_ykMlEHXr"
      },
      "source": [
        "gen_name = 'generator_model_160.h5'\n",
        "\n",
        "test_g = tf.keras.models.load_model(os.path.join(models_path, gen_name))\n",
        "samples, _ = generate_fake_samples(test_g, latent_dim, 16) \n",
        "n = 4\n",
        "samples = (samples + 1) /2.0\n",
        "# plot images\n",
        "for i in range(n * n):\n",
        "  # define subplot\n",
        "  pyplot.subplot(n, n, 1 + i)\n",
        "  # turn off axis\n",
        "  pyplot.axis('off')\n",
        "  # plot raw pixel data\n",
        "  pyplot.imshow(samples[i])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}